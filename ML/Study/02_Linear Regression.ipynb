{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Linear Regression(선형 회귀)",
   "id": "30d93576e7a57573"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "학습한다는 것은 : 어떤 선을 찾아야/그어야 데이터에 잘 맞을 것인가 찾아가는 것\n",
    "\n",
    "H(x) = Wx + b\n",
    "\n",
    "선의 모양은 W,b에 따라 달라짐\n",
    "\n",
    "어떤 Hypothesis가 좋냐를 판단할 때, 실제 y값과 Hypothesis의 값을 비교함 => Cost Function, Loss Function이라고 함\n",
    "\n",
    "Cost Function :  (H(x) - y)^2\n",
    "\n",
    "각 x 별 cost 계산 후 평균을 구함 => cost\n",
    "\n",
    "minimize cost(W,b) : cost 값을 최소화하는 W와 b를 구하는 것이 목표"
   ],
   "id": "de48c59d73c3a799"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### (1) Build graph using TF operations",
   "id": "d6b51f1677c95a39"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T13:49:29.294355Z",
     "start_time": "2025-12-10T13:49:29.251549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from jinja2.optimizer import optimize\n",
    "\n",
    "# X  and Y data\n",
    "x_train = [1,2,3]\n",
    "y_train = [1,2,3]\n",
    "\n",
    "W = tf.Variable(tf.random.normal([1]), name = 'weights')\n",
    "b = tf.Variable(tf.random.normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = x_train * W + b"
   ],
   "id": "6a80f4554d2a1640",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Cost 값 구하기",
   "id": "dc727e12da2629c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T13:49:29.302915Z",
     "start_time": "2025-12-10T13:49:29.298373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# cost/Loss Function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))"
   ],
   "id": "74e381ef2dac1fbd",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "GradientDescent",
   "id": "5f2ef8f1f27a74bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T13:49:29.312545Z",
     "start_time": "2025-12-10T13:49:29.307923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Minimize\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate = 0.01)\n",
    "# SGD = Stochastic Gradient Descent = 경사하강법"
   ],
   "id": "2b9e90fc6b786d68",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "학습 루프",
   "id": "a37e10754289d104"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T13:49:37.045076Z",
     "start_time": "2025-12-10T13:49:29.318555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for step in range(2001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 가설: y = W * x + b\n",
    "        hypothesis = W * x_train + b\n",
    "        # 비용함수 : MSE/Loss function/cost\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "    #W, b에 대한 Gradient  계산\n",
    "    grads = tape.gradient(cost,[W,b])\n",
    "    #Gradient를 이용해 W, b 업데이트\n",
    "    optimizer.apply_gradients(zip(grads,[W, b]))\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(step, \"cost:\", cost.numpy(), \"W:\", W.numpy(), \"b:\",b.numpy())"
   ],
   "id": "53bd37b99e85bb69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost: 36.449066 W: [-0.5699553] b: [-2.0761838]\n",
      "100 cost: 0.12004665 W: [1.4014285] b: [-0.91258526]\n",
      "200 cost: 0.07418154 W: [1.3155721] b: [-0.7173696]\n",
      "300 cost: 0.04583971 W: [1.2480685] b: [-0.563918]\n",
      "400 cost: 0.028326152 W: [1.1950045] b: [-0.443291]\n",
      "500 cost: 0.017503826 W: [1.153291] b: [-0.34846672]\n",
      "600 cost: 0.010816295 W: [1.1205007] b: [-0.27392656]\n",
      "700 cost: 0.006683796 W: [1.0947245] b: [-0.21533114]\n",
      "800 cost: 0.0041301786 W: [1.0744622] b: [-0.16927]\n",
      "900 cost: 0.0025522003 W: [1.0585339] b: [-0.13306144]\n",
      "1000 cost: 0.0015771015 W: [1.046013] b: [-0.10459833]\n",
      "1100 cost: 0.00097454776 W: [1.0361702] b: [-0.08222358]\n",
      "1200 cost: 0.0006022134 W: [1.0284331] b: [-0.06463526]\n",
      "1300 cost: 0.00037213112 W: [1.0223511] b: [-0.0508093]\n",
      "1400 cost: 0.00022995257 W: [1.0175699] b: [-0.03994063]\n",
      "1500 cost: 0.00014209574 W: [1.0138116] b: [-0.03139694]\n",
      "1600 cost: 8.780649e-05 W: [1.0108571] b: [-0.0246808]\n",
      "1700 cost: 5.4258566e-05 W: [1.0085347] b: [-0.01940131]\n",
      "1800 cost: 3.3528664e-05 W: [1.006709] b: [-0.01525131]\n",
      "1900 cost: 2.0719252e-05 W: [1.0052739] b: [-0.01198893]\n",
      "2000 cost: 1.2803212e-05 W: [1.0041459] b: [-0.00942436]\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## GradientTape\n",
    "\n",
    "자동 미분을 통해 동적으로 Gradient 값을 확인해 볼 수 있다는 장점이 있음\n",
    "\n",
    "텐서플로는 자동 미분(주어진 입력 변수에 대한 연산의 그래디언트(gradient)를 계산하는 것) 을 위한 tf.GradientTape API를 제공\n",
    "\n",
    "tf.GradientTape는 컨텍스트(context) 안에서 실행된 모든 연산을 테이프(tape)에 \"기록\"\n",
    "\n",
    "그 다음 텐서플로는 후진 방식 자동 미분(reverse mode differentiation)을 사용해 테이프에 \"기록된\" 연산의 그래디언트를 계산\n",
    "\n"
   ],
   "id": "6593a2a16d5d5001"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T14:10:16.972962Z",
     "start_time": "2025-12-10T14:10:03.838142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_str = input(\"x 값을 공백으로 입력해주세요(예 : 1 2 3): \")\n",
    "y_str = input(\"y 값을 공백으로 입력해주세요(예 : 4 5 6): \")\n",
    "\n",
    "x_list = list(map(int, x_str.split(\" \")))\n",
    "y_list = list(map(int, y_str.split(\" \")))\n",
    "\n",
    "if len(x_list) != len(y_list):\n",
    "    raise ValueError(\"x와 y의 개수가 같아야 합니다.\")\n",
    "\n",
    "W = tf.Variable(tf.random.normal([1]), name = 'weights')\n",
    "b = tf.Variable(tf.random.normal([1]), name = 'bias')\n",
    "\n",
    "x_data = tf.constant(x_list, dtype = tf.float32)\n",
    "y_data = tf.constant(y_list, dtype = tf.float32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate = 0.01)\n",
    "\n",
    "for step in range(2001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = W * x_data + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "    #W,b에 대한 Gradient 계산\n",
    "    grads = tape.gradient(cost,[W,b])\n",
    "    #graident를 이용해 W,b 업데이트\n",
    "    optimizer.apply_gradients(zip(grads,[W,b]))\n",
    "\n",
    "    if step % 100 ==0:\n",
    "        print(step, \"cost:\", cost.numpy(), \"W:\", W.numpy(), \"b:\",b.numpy())\n",
    "\n",
    "# 학습 끝난 후 예측해보기\n",
    "x_test_str = input(\"예측하고 싶은 x 값들을 공백으로 입력:\" )\n",
    "x_test_list = list(map(int, x_test_str.split(\" \")))\n",
    "x_test_tensor = tf.constant(x_test_list, dtype = tf.float32)\n",
    "\n",
    "y_pred = W * x_test_tensor + b\n",
    "print(\"입력한 x: \", x_test_list)\n",
    "print(\"예측한 y: \", y_pred)\n",
    "\n"
   ],
   "id": "34081c5fcac33618",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost: 586.78467 W: [0.42445678] b: [-0.9217541]\n",
      "100 cost: 0.6770739 W: [9.046549] b: [2.1672406]\n",
      "200 cost: 0.41839007 W: [9.250552] b: [1.7036716]\n",
      "300 cost: 0.25853923 W: [9.410867] b: [1.3392402]\n",
      "400 cost: 0.15976106 W: [9.536887] b: [1.0527639]\n",
      "500 cost: 0.09872282 W: [9.635952] b: [0.82756793]\n",
      "600 cost: 0.061004426 W: [9.713826] b: [0.6505422]\n",
      "700 cost: 0.037696842 W: [9.775041] b: [0.51138586]\n",
      "800 cost: 0.023294555 W: [9.823161] b: [0.40199584]\n",
      "900 cost: 0.014394385 W: [9.86099] b: [0.31600332]\n",
      "1000 cost: 0.0088947965 W: [9.890725] b: [0.24840659]\n",
      "1100 cost: 0.00549645 W: [9.914101] b: [0.19527037]\n",
      "1200 cost: 0.0033965122 W: [9.932475] b: [0.15350035]\n",
      "1300 cost: 0.0020988777 W: [9.9469185] b: [0.12066617]\n",
      "1400 cost: 0.0012969421 W: [9.958273] b: [0.09485454]\n",
      "1500 cost: 0.0008014426 W: [9.967198] b: [0.07456483]\n",
      "1600 cost: 0.00049526006 W: [9.974215] b: [0.05861543]\n",
      "1700 cost: 0.00030605224 W: [9.97973] b: [0.04607829]\n",
      "1800 cost: 0.0001891451 W: [9.984065] b: [0.03622288]\n",
      "1900 cost: 0.00011689615 W: [9.987473] b: [0.02847683]\n",
      "2000 cost: 7.226297e-05 W: [9.9901495] b: [0.02238873]\n",
      "입력한 x:  [4]\n",
      "예측한 y:  tf.Tensor([39.982986], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6a55626d190cc26f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
